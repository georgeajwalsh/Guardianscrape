{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Guardian API Key and Base URL\n",
    "API_KEY = \"998344a2-04a1-4410-9d53-1490cfa2e9d2\"\n",
    "BASE_URL = \"https://content.guardianapis.com/search\"\n",
    "\n",
    "# Output file\n",
    "FILE_NAME = \"guardian_articles.csv\"\n",
    "\n",
    "# Number of weeks to scrape\n",
    "NUM_WEEKS = 500\n",
    "ARTICLES_PER_WEEK = 2\n",
    "\n",
    "# Generate random weekly dates (going backward from today)\n",
    "start_date = datetime.today()\n",
    "dates = [(start_date - timedelta(weeks=i)).strftime(\"%Y-%m-%d\") for i in range(NUM_WEEKS)]\n",
    "\n",
    "# Load existing data if the file exists\n",
    "if os.path.exists(FILE_NAME):\n",
    "    df = pd.read_csv(FILE_NAME)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"headline\", \"publication_date\", \"url\"])\n",
    "\n",
    "# Fetch articles for each week\n",
    "for week_date in dates:\n",
    "    params = {\n",
    "        \"api-key\": API_KEY,\n",
    "        \"from-date\": week_date,\n",
    "        \"to-date\": week_date,\n",
    "        \"show-fields\": \"headline\",\n",
    "        \"page-size\": 10,  # Get up to 10 articles from the date\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        articles = data[\"response\"][\"results\"]\n",
    "        \n",
    "        if articles:\n",
    "            selected_article = random.choice(articles)  # Pick one randomly\n",
    "            article_data = {\n",
    "                \"headline\": selected_article[\"webTitle\"],\n",
    "                \"publication_date\": selected_article[\"webPublicationDate\"],\n",
    "                \"url\": selected_article[\"webUrl\"],\n",
    "            }\n",
    "\n",
    "            # Append new data and save\n",
    "            df = pd.concat([df, pd.DataFrame([article_data])], ignore_index=True)\n",
    "            df.to_csv(FILE_NAME, index=False)\n",
    "\n",
    "            print(f\"Saved article from {week_date}: {article_data['headline']}\")\n",
    "        else:\n",
    "            print(f\"No articles found for {week_date}.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for {week_date}: {response.status_code}\")\n",
    "\n",
    "print(\"âœ… Data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this allows us to test the sucess of our scrapinng and ensure it has gone back 500 weeks also that it was effectively formatted\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
